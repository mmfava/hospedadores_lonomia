{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching core tidyverse packages\u001b[22m ──────────────────────── tidyverse 2.0.0 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdplyr    \u001b[39m 1.1.4     \u001b[32m✔\u001b[39m \u001b[34mreadr    \u001b[39m 2.1.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mforcats  \u001b[39m 1.0.0     \u001b[32m✔\u001b[39m \u001b[34mstringr  \u001b[39m 1.5.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2  \u001b[39m 3.4.4     \u001b[32m✔\u001b[39m \u001b[34mtibble   \u001b[39m 3.2.1\n",
      "\u001b[32m✔\u001b[39m \u001b[34mlubridate\u001b[39m 1.9.3     \u001b[32m✔\u001b[39m \u001b[34mtidyr    \u001b[39m 1.3.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mpurrr    \u001b[39m 1.0.2     \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mpurrr\u001b[39m::\u001b[32mmap()\u001b[39m    masks \u001b[34mmaps\u001b[39m::map()\n",
      "\u001b[36mℹ\u001b[39m Use the conflicted package (\u001b[3m\u001b[34m<http://conflicted.r-lib.org/>\u001b[39m\u001b[23m) to force all conflicts to become errors\n",
      "\n",
      "Attaching package: ‘janitor’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    chisq.test, fisher.test\n",
      "\n",
      "\n",
      "Linking to GEOS 3.12.1, GDAL 3.8.3, PROJ 9.3.1; sf_use_s2() is TRUE\n",
      "\n",
      "\n",
      "Attaching package: ‘cowplot’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:lubridate’:\n",
      "\n",
      "    stamp\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "R version 4.3.2 (2023-10-31)\n",
       "Platform: x86_64-conda-linux-gnu (64-bit)\n",
       "Running under: Ubuntu 22.04.3 LTS\n",
       "\n",
       "Matrix products: default\n",
       "BLAS/LAPACK: /opt/conda/lib/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0\n",
       "\n",
       "locale:\n",
       " [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n",
       " [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n",
       " [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n",
       " [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n",
       " [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n",
       "[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n",
       "\n",
       "time zone: Etc/UTC\n",
       "tzcode source: system (glibc)\n",
       "\n",
       "attached base packages:\n",
       "[1] stats     graphics  grDevices utils     datasets  methods   base     \n",
       "\n",
       "other attached packages:\n",
       " [1] cowplot_1.1.3           eks_1.0.4               sp_2.1-3               \n",
       " [4] sf_1.0-15               readxl_1.4.3            leaflet_2.2.1          \n",
       " [7] janitor_2.2.0           lubridate_1.9.3         forcats_1.0.0          \n",
       "[10] stringr_1.5.1           dplyr_1.1.4             purrr_1.0.2            \n",
       "[13] readr_2.1.4             tidyr_1.3.0             tibble_3.2.1           \n",
       "[16] ggplot2_3.4.4           tidyverse_2.0.0         maps_3.4.2             \n",
       "[19] CoordinateCleaner_3.0.1\n",
       "\n",
       "loaded via a namespace (and not attached):\n",
       " [1] tidyselect_1.2.0    IRdisplay_1.1       fastmap_1.1.1      \n",
       " [4] lazyeval_0.2.2      pracma_2.4.4        digest_0.6.33      \n",
       " [7] timechange_0.2.0    lifecycle_1.0.4     terra_1.7-71       \n",
       "[10] magrittr_2.0.3      compiler_4.3.2      rlang_1.1.2        \n",
       "[13] tools_4.3.2         utf8_1.2.4          data.table_1.14.10 \n",
       "[16] htmlwidgets_1.6.4   mclust_6.0.1        classInt_0.4-10    \n",
       "[19] plyr_1.8.9          xml2_1.3.6          repr_1.1.6         \n",
       "[22] KernSmooth_2.23-22  pbdZMQ_0.3-10       withr_2.5.2        \n",
       "[25] mapsf_0.9.0         grid_4.3.2          fansi_1.0.6        \n",
       "[28] e1071_1.7-14        colorspace_2.1-0    scales_1.3.0       \n",
       "[31] isoband_0.2.7       cli_3.6.2           mvtnorm_1.2-4      \n",
       "[34] crayon_1.5.2        generics_0.1.3      rgbif_3.7.9        \n",
       "[37] httr_1.4.7          tzdb_0.4.0          DBI_1.2.0          \n",
       "[40] proxy_0.4-27        rnaturalearth_1.0.1 s2_1.1.6           \n",
       "[43] cellranger_1.1.0    base64enc_0.1-3     vctrs_0.6.5        \n",
       "[46] Matrix_1.6-4        jsonlite_1.8.8      hms_1.1.3          \n",
       "[49] crosstalk_1.2.1     maplegend_0.1.0     units_0.8-5        \n",
       "[52] geosphere_1.5-18    glue_1.6.2          codetools_0.2-19   \n",
       "[55] stringi_1.8.3       gtable_0.3.4        munsell_0.5.0      \n",
       "[58] pillar_1.9.0        htmltools_0.5.7     IRkernel_1.3.2     \n",
       "[61] R6_2.5.1            wk_0.9.1            ks_1.14.2          \n",
       "[64] evaluate_0.23       oai_0.4.0           lattice_0.22-5     \n",
       "[67] snakecase_0.11.1    class_7.3-22        Rcpp_1.0.11        \n",
       "[70] uuid_1.1-1          whisker_0.4.1       pkgconfig_2.0.3    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Packages --------------------------------------------------------------------\n",
    "library(CoordinateCleaner) # clean coordinartes\n",
    "library(maps)\n",
    "library(tidyverse) # data manipulation and visualization\n",
    "library(janitor) # clear table column name\n",
    "library(leaflet)\n",
    "library(readxl)\n",
    "library(sf)                                                                         \n",
    "library(sp)\n",
    "library(eks)\n",
    "library(cowplot)\n",
    "\n",
    "## Negate funcion ---------------------------------------------------------------\n",
    "`%!in%` = Negate(`%in%`)\n",
    "\n",
    "## Session info ------------------------------------------------------------------\n",
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing 67 addresses to the Nominatim single address geocoder\n",
      "\n",
      "Query completed in: 67.1 seconds\n",
      "\n",
      "\u001b[1m\u001b[22mNew names:\n",
      "\u001b[36m•\u001b[39m `lat` -> `lat...7`\n",
      "\u001b[36m•\u001b[39m `long` -> `long...8`\n",
      "\u001b[36m•\u001b[39m `lat` -> `lat...18`\n",
      "\u001b[36m•\u001b[39m `long` -> `long...19`\n"
     ]
    }
   ],
   "source": [
    "## Import and Preprocess Lonomia Data \n",
    "# Importing the data from a Google Spreadsheet using the rio package\n",
    "df_lonomia <- rio::import(\"https://docs.google.com/spreadsheets/d/1PzJdUGWxhmMIk8-iGw9YqTbpHNNTPvCMUJOnHefPZD0/edit?usp=sharing\") |> \n",
    "  # Cleaning column names for better readability and format consistency \n",
    "  clean_names() |> \n",
    "  # Recode certain species names in the 'specie_orig' column ('Lonomia diabolus' to 'Lonomia achelous')\n",
    "  mutate(specie_orig = recode(specie_orig, \"Lonomia diabolus\" = \"Lonomia achelous\")) |>\n",
    "  # Filter rows based on the presence of specific substrings in the 'specie_orig' column\n",
    "  filter(str_detect(specie_orig, \"achelous|paraobliqua|obliqua\")) |> \n",
    "  # Remove the 'specie' column from the data frame\n",
    "  select(-specie) |> \n",
    "  # Rename the 'specie_orig' column to 'species'\n",
    "  rename(species=specie_orig) |> \n",
    "  # Apply the next mutations on a per-row basis\n",
    "  rowwise() |> \n",
    "  # Create a new 'addrs' column: concatenates different location-related fields if 'use_coordinates' is 'possible', else it's NA\n",
    "  mutate(addrs = if_else(\n",
    "    use_coordinates %in% \"possible\", \n",
    "    gsub(\"^,+(,+|s|-)*\", \"\", paste(location, municipality_department, province_state, country, sep = \", \")),\n",
    "    NA)) |> \n",
    "  # Perform geocoding on the 'addrs' field using OpenStreetMap (osm) method via tidygeocoder\n",
    "  tidygeocoder::geocode(addrs, method = \"osm\")\n",
    "\n",
    "## Process Geocoding for df_lonomia dataframe\n",
    "# The following part of the script involves geocoding operations on the 'df_lonomia' dataframe\n",
    "\n",
    "# Rename `lat...7` column to 'lat' and `long...8` to 'long'\n",
    "df_lonomia <- df_lonomia |> \n",
    "  rename(lat = `lat...7`, long = `long...8`) |> \n",
    "  # Replace missing lat and long values with corresponding values from `lat...18` and `long...19`\n",
    "  mutate(\n",
    "    lat = if_else(is.na(lat) & !is.na(`lat...18`), `lat...18`, lat),\n",
    "    long = if_else(is.na(long) & !is.na(`long...19`), `long...19`, long)\n",
    "  ) |>\n",
    "  # Add a new column 'coord_model' which is 1 if both lat and long are not NA, otherwise it's 0\n",
    "  mutate(coord_model = if_else(!is.na(lat) & !is.na(long), 1, 0)) |> \n",
    "  # Perform geocoding on city, state, and country columns using the OpenStreetMap (osm) method via tidygeocoder\n",
    "  tidygeocoder::geocode(city = municipality_department, state = province_state, country = country, method = \"osm\") |> \n",
    "  # Rename the resulting `lat...7` and `long...8` back to 'lat' and 'long'\n",
    "  rename(lat = `lat...7`, long = `long...8`) |> \n",
    "  # Again replaces the missing lat and long values now with the corresponding values from `lat...21` and `long...22`\n",
    "  mutate(\n",
    "    lat = if_else(is.na(lat) & !is.na(`lat...21`), `lat...21`, lat),\n",
    "    long = if_else(is.na(long) & !is.na(`long...22`), `long...22`, long)\n",
    "  ) |>\n",
    "  # Perform reverse geocoding on 'lat' and 'long' fields using OpenStreetMap (osm) method via tidygeocoder\n",
    "  # full_results=TRUE gets the complete results from reverse geocoding \n",
    "  tidygeocoder::reverse_geocode(lat = 'lat', long = 'long', method = \"osm\", full_results=TRUE) |> \n",
    "  # Selecting the necessary columns from the data frame\n",
    "  select(\n",
    "    species, coord_model, lat = `lat...7`, long = `long...8`, month, year, municipality, \n",
    "    county, city, town, state, country = `country...6`, country_code, iso3 = `ISO3166-2-lvl4`,\n",
    "    address, boundingbox, taxonomic_identification_type, taxnomer_who_identified, \n",
    "    reference, reference_type, more_info) |> \n",
    "  # Filter dataframe to keep only those cases where both 'lat' and 'long' are available\n",
    "  filter(complete.cases(lat, long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Coordinate Cleaning Process on the 'df_lonomia' data frame\n",
    "\n",
    "# Use CoordinateCleaner to clean up and validate geographic coordinates in the dataframe\n",
    "\n",
    "# clean_coordinates function is used to perform various tests and corrections on geographic coordinate datasets\n",
    "# species: Column name containing species names, in this case, it's \"species\"\n",
    "# lon: Column name containing longitude values, in this case, it's \"long\"\n",
    "# lat: Column name containing latitude values, in this case, it's \"lat\"\n",
    "# tests: The tests to be applied. In this case 'equal', 'seas', and 'zeros' tests are applied\n",
    "  # equal: Checks for records where latitude equals longitude\n",
    "  # seas: Checks for records located in the sea\n",
    "  # zeros: Checks for records with zero as coordinates\n",
    "# value: Determines if the cleaned or original dataset should be returned. Here we specify 'clean' so that the cleaned dataset will be returned\n",
    "# report: If TRUE, it generates a report of performed tests, otherwise not. Here it is set to FALSE, so no report will be generated\n",
    "dt_clean <- df_lonomia |> \n",
    "  CoordinateCleaner::clean_coordinates(\n",
    "    species = \"species\", \n",
    "    lon = \"long\", lat = \"lat\", \n",
    "    tests = c(\"equal\", \"seas\", \"zeros\"), \n",
    "    value = \"clean\", report = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Transforming and Cleaning Process on 'dt_clean' data frame\n",
    "# Begin transformation with the cleaned data in 'dt_clean'\n",
    "lonomie <- dt_clean |>\n",
    "  # 'select' is used to select columns 'species', 'long', and 'lat' from the data\n",
    "  select(species, long, lat) |>\n",
    "  # 'distinct' is used to remove duplicates from the data \n",
    "  # '.keep_all = true' means all columns are retained\n",
    "  distinct(.keep_all = true) |>\n",
    "  # 'st_as_sf' converts simple data frames to simple feature objects\n",
    "  # 'coords = c(\"long\", \"lat\")' specifies which columns contain the coordinate data\n",
    "  # 'crs' - Coordinate Reference System, \"+proj=longlat +datum=WGS84 +no_defs\" is a commonly used CRS \n",
    "  st_as_sf(coords = c(\"long\", \"lat\"), crs = \"+proj=longlat +datum=WGS84 +no_defs\")\n",
    "\n",
    "# Updating 'dt_clean' by converting it into a simple feature object\n",
    "# This will be useful for spatial analysis or creating spatial visualizations\n",
    "dt_clean <- dt_clean |> \n",
    "  # Use 'st_as_sf' to convert the dataframe into an sf (simple features) object\n",
    "  # 'coords = c(\"long\", \"lat\")' specifies which columns contain the coordinate data\n",
    "  # 'crs' - Coordinate Reference System, \"+proj=longlat +datum=WGS84 +no_defs\" is a commonly used CRS\n",
    "  st_as_sf(coords = c(\"long\", \"lat\"), crs = \"+proj=longlat +datum=WGS84 +no_defs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 'read.csv2' is used to read the file 'occ.csv' from the specified path\n",
    "# 'here::here' function constructs paths relative to your project's root directory\n",
    "occ <- read.csv2(here::here(\"data\", \"occ.csv\")) \n",
    "\n",
    "# Read an excel sheet named 'survey_info_hosts' from the supplementary material using 'read_excel'\n",
    "native_hst <- read_excel(here::here(\"data\", \"hosts.xlsx\"), sheet = \"survey_info_hosts\") |> \n",
    "  # Use 'select' to keep only the desired columns: 'host_complete_name', 'native', and 'lonomia_species'\n",
    "  select(host_complete_name, native, lonomia_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading layer `South_America' from data source \n",
      "  `/home/jovyan/work/hospedadores_lonomia/data/South_America' \n",
      "  using driver `ESRI Shapefile'\n",
      "Simple feature collection with 15 features and 1 field\n",
      "Geometry type: MULTIPOLYGON\n",
      "Dimension:     XY\n",
      "Bounding box:  xmin: -109.4461 ymin: -58.49861 xmax: -26.24139 ymax: 12.59028\n",
      "Geodetic CRS:  WGS 84\n"
     ]
    }
   ],
   "source": [
    "# Loading Spatial Data and Merging Datasets\n",
    "# 'st_read' is used to read spatial data from the specified path\n",
    "# 'st_transform' function is applied to transform the spatial data to a common coordinate reference system (CRS), here CRS is 4326 which represents WGS 84 -- WGS84 - World Geodetic System 1984, used in GPS\n",
    "south <- st_read(dsn = here::here(\"data\", \"South_America\")) |>\n",
    "            st_transform(4326)\n",
    "\n",
    "## Merge 'occ' and 'native_hst' datasets and classify as native or exotic \n",
    "# We start with the 'occ' dataset and use 'left_join' to add data from 'native_hst'\n",
    "# The join is performed on matching the 'occ' dataset's 'Taxon_name' column with 'native_hst' dataset's 'host_complete_name' column\n",
    "# Use 'mutate' to add a new column 'native'. If the original 'native' value was \"yes\", it is replaced with \"Native\", otherwise, it is replaced with \"Exotic\"\n",
    "dt_maps <- occ |> \n",
    "    left_join(native_hst, by = c(\"Taxon_name\" = \"host_complete_name\")) |>\n",
    "    mutate(native = ifelse(native == \"yes\", \"Native\", \"Exotic\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“attribute variables are assumed to be spatially constant throughout all geometries”\n",
      "Warning message:\n",
      "“attribute variables are assumed to be spatially constant throughout all geometries”\n"
     ]
    }
   ],
   "source": [
    "## denstiy kernell -------------------------------------------------------------\n",
    "## based on:\n",
    "## https://cran.r-project.org/web/packages/eks/vignettes/tidysf_kde.html\n",
    "kern_maps <- list()\n",
    "\n",
    "for(i in c(\"achelous\", \"obliqua\")){\n",
    " \n",
    "  ## density map (sf)\n",
    "  krnlm <- dt_maps |> \n",
    "    filter(str_detect(native, \"Native\") &\n",
    "               str_detect(lonomia_species, i)) |>\n",
    "    select(Longitude, Latitude) |>\n",
    "    as_tibble() |>\n",
    "    st_as_sf(coords = c(\"Longitude\", \"Latitude\"),\n",
    "    crs = \"+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0\") |>\n",
    "    st_kde()\n",
    "  \n",
    "  maps <- krnlm$sf |>\n",
    "    st_intersection(south) |>\n",
    "    mutate(contlabel = as.numeric(contlabel))\n",
    "    \n",
    "  kern_maps[[paste0(i, \"_native\")]] <- maps\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a list of Maps using ggplot\n",
    "# Create an empty list `ggplants` to store the plots\n",
    "ggplants <- list()\n",
    "\n",
    "# Loop through each of the names in `kern_maps`\n",
    "for(i in names(kern_maps)){\n",
    "    # For each name, create a plot using ggplot\n",
    "    figs_plan <- ggplot() +\n",
    "        # Plot the south american spatial data with grey color\n",
    "        geom_sf(data = south, fill = \"grey80\") + \n",
    "        # Overlay this with the specific map from kern_maps for the current index name 'i'\n",
    "        # The fill parameter is set to the contlabel attribute of the dataset\n",
    "        geom_sf(data = kern_maps[[i]], \n",
    "                aes(fill = contlabel), \n",
    "                alpha = 0.8, color = NA, show.legend = FALSE) +  \n",
    "        # Apply gradient color scale to fill based on the value of contlabel attribute\n",
    "        scale_fill_gradient2(low = \"#4EA699\",  # Color for low values\n",
    "                             mid = \"white\", midpoint = 50,  # Color for mid-range values\n",
    "                             high = \"#ff1b6b\",  # Color for high values\n",
    "                             name = '%') +  # Name of legend\n",
    "        # Add the borders of South American countries back on top, colored grey\n",
    "        geom_sf(data = south, fill = NA, color = \"grey50\", size = 1.3) +\n",
    "        # Set the limits for x and y coordinates\n",
    "        xlim(c(-90, -30)) +\n",
    "        ylim(c(-55, 12)) +  \n",
    "        # Use a light theme for the plot\n",
    "        theme_light() +\n",
    "        # Remove axis title, test and ticks for a clean look\n",
    "        theme(axis.title=element_blank(),\n",
    "            axis.text=element_blank(),\n",
    "            axis.ticks=element_blank())\n",
    "    \n",
    "    # Add the plot to the list of plots\n",
    "    ggplants[[i]] <- figs_plan\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Reading and transforming data points\n",
    "# Read the 'H2' sheet from lonomism_southamerica.xlsx dataset located in Scripts/datasets folder \n",
    "data_points <- read_excel(here::here(\"data\", \"lonomism_southamerica.xlsx\"), sheet = \"H2\") |>\n",
    "  # Select columns 'long' and 'lat'\n",
    "  dplyr::select(long, lat) |>\n",
    "  # Rename these columns to 'x' and 'y' respectively\n",
    "  dplyr::rename(y = lat, x = long) |>\n",
    "  # Convert dataframe into spatial points\n",
    "  SpatialPoints() |>\n",
    "  # Transform spatial points to sf object (Simple Features - modern standard for spatial vector data)\n",
    "  st_as_sf() |> \n",
    "  # Create a buffer around each point with distance of 0.5 degrees \n",
    "  st_buffer(dist = 0.5) |> # ! 0.5 degrees\n",
    "  # Combine all these buffers into a single shape\n",
    "  st_union()\n",
    "\n",
    "\n",
    "# Assigning a Coordinate Reference System (CRS)\n",
    "# Set the CRS of the data points to WGS 84 (code: 4326), defining this spatial object to use a global latitude-longitude grid\n",
    "st_crs(data_points) <- 4326"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
